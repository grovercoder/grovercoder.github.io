<!DOCTYPE html>
<html lang="en">

<head>
  <title>
  Using Ollama locally with Python · GroverCoder
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="GroverCoder">
<meta name="description" content="
  Overview
  
    
    Link to heading
  

AI and LLMs are all the rage these days.  Incorporating them into your code can open up a number of possibilities.  Using the online resources / APIs have some issues though:

your code may need to survive even if the online resource changes or is no longer available
your code may be rate limited or blocked if thresholds are exceeded
the token cost may accummulate much quicker than you were expecting.  In practice that $0.0012 process is fine, once you go live you might find though that that process is run millions of times in a short time frame. (Perhaps this is a good problem to have?)
there may be legal issues passing your data or intellectual property to an upstream provider who is not always clear how that data may be used.

Sometimes running a LLM process locally is a better choice.">
<meta name="keywords" content="blog,developer,personal">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Using Ollama locally with Python">
  <meta name="twitter:description" content=" Overview Link to heading AI and LLMs are all the rage these days. Incorporating them into your code can open up a number of possibilities. Using the online resources / APIs have some issues though:
your code may need to survive even if the online resource changes or is no longer available your code may be rate limited or blocked if thresholds are exceeded the token cost may accummulate much quicker than you were expecting. In practice that $0.0012 process is fine, once you go live you might find though that that process is run millions of times in a short time frame. (Perhaps this is a good problem to have?) there may be legal issues passing your data or intellectual property to an upstream provider who is not always clear how that data may be used. Sometimes running a LLM process locally is a better choice.">

<meta property="og:url" content="https://grovercoder.github.io/posts/2024108-local-ollama-python/">
  <meta property="og:site_name" content="GroverCoder">
  <meta property="og:title" content="Using Ollama locally with Python">
  <meta property="og:description" content=" Overview Link to heading AI and LLMs are all the rage these days. Incorporating them into your code can open up a number of possibilities. Using the online resources / APIs have some issues though:
your code may need to survive even if the online resource changes or is no longer available your code may be rate limited or blocked if thresholds are exceeded the token cost may accummulate much quicker than you were expecting. In practice that $0.0012 process is fine, once you go live you might find though that that process is run millions of times in a short time frame. (Perhaps this is a good problem to have?) there may be legal issues passing your data or intellectual property to an upstream provider who is not always clear how that data may be used. Sometimes running a LLM process locally is a better choice.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-10-09T16:01:10-06:00">
    <meta property="article:modified_time" content="2024-10-09T16:01:10-06:00">
    <meta property="article:tag" content="AI / LLMs">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="Application Design">




<link rel="canonical" href="https://grovercoder.github.io/posts/2024108-local-ollama-python/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.b04b3b293fda1eeaad08ca369d0beb7106ec93f453ce9a8a3b787a21b76e7df6.css" integrity="sha256-sEs7KT/aHuqtCMo2nQvrcQbsk/RTzpqKO3h6IbduffY=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.38bd245a7922e87128d12aa12859e32c3455413c05485c304123a6af4db69763.css" integrity="sha256-OL0kWnki6HEo0SqhKFnjLDRVQTwFSFwwQSOmr022l2M=" crossorigin="anonymous" media="screen" />
  



 


  
  
    
    <link rel="stylesheet" href="/scss/projects.min.9b320a6c6cb302209029718d86a3057d0a0a38d5db23cda6664562857e6398d1.css" integrity="sha256-mzIKbGyzAiCQKXGNhqMFfQoKONXbI82mZkVihX5jmNE=" crossorigin="anonymous" media="screen" />
  



<link rel="icon" type="image/svg+xml" href="/img/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/img/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/img/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="https://grovercoder.github.io/">
      GroverCoder
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://grovercoder.github.io/posts/2024108-local-ollama-python/">
              Using Ollama locally with Python
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2024-10-09T16:01:10-06:00">
                October 9, 2024
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              11-minute read
            </span>
          </div>
          
          
          <div class="tags">
  <i class="fa-solid fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/ai-/-llms/">AI / LLMs</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/python/">Python</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/application-design/">Application Design</a>
    </span></div>

        </div>
      </header>

      
      <div class="tableofcontents">
        <nav id="TableOfContents">
  <ul>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#know-your-ollama-host">Know your Ollama Host</a></li>
    <li><a href="#connecting-to-ollama-with-python">Connecting to Ollama with Python</a>
      <ul>
        <li><a href="#the-rest-api-approach">The Rest API approach:</a></li>
        <li><a href="#the-python-library-approach">The Python Library approach</a></li>
      </ul>
    </li>
    <li><a href="#downsides-to-hosting-locally">Downsides to hosting locally</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
      </div>
      

      <div class="post-content">
        
        <h2 id="overview">
  Overview
  <a class="heading-link" href="#overview">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>AI and LLMs are all the rage these days.  Incorporating them into your code can open up a number of possibilities.  Using the online resources / APIs have some issues though:</p>
<ul>
<li>your code may need to survive even if the online resource changes or is no longer available</li>
<li>your code may be rate limited or blocked if thresholds are exceeded</li>
<li>the token cost may accummulate much quicker than you were expecting.  In practice that $0.0012 process is fine, once you go live you might find though that that process is run millions of times in a short time frame. (Perhaps this is a good problem to have?)</li>
<li>there may be legal issues passing your data or intellectual property to an upstream provider who is not always clear how that data may be used.</li>
</ul>
<p>Sometimes running a LLM process locally is a better choice.</p>
<p>When considering local LLMs, you quickly discover <a href="https://ollama.com/"  class="external-link" target="_blank" rel="noopener">Ollama</a>.  And you quickly discover that interacting with Ollama with code is not always quite the way the docs say it is.  The docs and tutorials can become outdated in short order.  That applies to this article as well.  What works for me today may not work for you by the time you read this.  And with that disclaimer out of the way, let us look at this.</p>
<p>For this article we will assume you have Ollama installed, running, and providing one or more models for use.  You can run <code>ollama list</code> on your ollama server to see the list of available models.  If you do not have a model, check <a href="https://ollama.com/library"  class="external-link" target="_blank" rel="noopener">Ollama&rsquo;s Models Page</a>, and then run <code>ollama pull MODEL_NAME</code> for your chosen model.</p>
<h2 id="know-your-ollama-host">
  Know your Ollama Host
  <a class="heading-link" href="#know-your-ollama-host">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>If your python code is not running on the same physical box as your Ollama server, you&rsquo;ll need to indicate where to find your Ollama server.  How you do this depends on what code method you are attempting.</p>
<ol>
<li>
<p>Environment Variable</p>
<p>In most cases setting an environment variable before running your code can resolve the connection issue:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>export OLLAMA_HOST<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;192.168.123.123&#34;</span>    <span style="color:#75715e"># Adjust the server&#39;s IP. </span>
</span></span></code></pre></div><p>In some cases you can include the port number here so &ldquo;192.168.123.123:11434&rdquo;, for example.</p>
<p>Setting this only takes effect for the duration of the shell you are in.  You may need to ensure this variable is set in a different manner if you need it to survive closing your shell or rebooting.</p>
<p>A better option might be to set this directly before call your application:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>OLLAMA_HOST<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;192.168.123.123:11434&#34;</span> python3 yourapplication.py
</span></span></code></pre></div></li>
<li>
<p>Make your code aware of the host.</p>
<p>I find I often have to include other configuration variables, like a database host, port, and credentials.  I usually use the <a href="https://pypi.org/project/python-dotenv/"  class="external-link" target="_blank" rel="noopener">python-dotenv</a> package for this.  (Or just create a <code>config.py</code> file, set my values there, and import it into my code as needed.)</p>
<ol>
<li>
<p>Install the dotenv library with <code>pip install python-dotenv</code></p>
</li>
<li>
<p>create or edit your <code>.env</code> file in your python project folder:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span># /your/python/project/.env
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>OLLAMA_HOST=&#34;192.168.123.123:11434&#34;
</span></span></code></pre></div><blockquote>
<p><strong>WARNING</strong>: if your <code>.env</code> file contains sensitive information like API keys, connection details, passwords, etc. then you should <strong>NOT</strong> include this file in your source control.  The <code>.env</code> file is routinely excluded via <code>.gitignore</code> for these reasons.</p>
</blockquote>
</li>
<li>
<p>Now call the load_dotenv() method as soon as you can in your python code.  This will find your <code>.env</code> file, and create temporary environment variables you can then use in your code:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> dotenv <span style="color:#f92672">import</span> load_dotenv
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>load_dotenv()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#39;OLLAMA_HOST&#39;</span>))
</span></span></code></pre></div></li>
</ol>
</li>
</ol>
<p>I believe the second option is the more robust solution, but your specific needs will dictate which approach is most appropriate.</p>
<h2 id="connecting-to-ollama-with-python">
  Connecting to Ollama with Python
  <a class="heading-link" href="#connecting-to-ollama-with-python">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>There are two ways to connect to your Ollama server that I have found.</p>
<ol>
<li>Using the REST API,</li>
<li>Via a python module such as the <a href="https://pypi.org/project/ollama/"  class="external-link" target="_blank" rel="noopener">Ollama Library</a> or <a href="https://python.langchain.com/docs/integrations/chat/ollama/"  class="external-link" target="_blank" rel="noopener">LangChain</a></li>
</ol>
<p>In either case, it is assumed you have Ollama installed, running, and providing one or more models for use.  You can run <code>ollama list</code> on your ollama server to see the list of available models.  If you do not have a model, check <a href="https://ollama.com/library"  class="external-link" target="_blank" rel="noopener">Ollamas Models Page</a>, and then run <code>ollama pull MODEL_NAME</code> for your chosen model.</p>
<h3 id="the-rest-api-approach">
  The Rest API approach:
  <a class="heading-link" href="#the-rest-api-approach">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Using the REST API simply calls a URL, passing along the information needed.  To do so you&rsquo;ll need to be able to issue a web request.  In Python this is usually handled with the <code>requests</code> library.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># create an example file to work with, with these contents.  i.e. ollama_test.py</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Make sure the file is in an environment that has requests and python-dotenv installed</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> json
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> requests
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> dotenv <span style="color:#f92672">import</span> load_dotenv
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">ask_ollama</span>(question, ollama_host<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;127.0.0.1:11434&#34;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> ollama_host:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#39;could not determine the OLLAMA_HOST - trying the local URL&#39;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Create the URL string for the REST request</span>
</span></span><span style="display:flex;"><span>    ollama_url <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;http://</span><span style="color:#e6db74">{</span>ollama_host<span style="color:#e6db74">}</span><span style="color:#e6db74">/api/generate&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># define the prompt or question we are going to ask of the ollama server</span>
</span></span><span style="display:flex;"><span>    prompt <span style="color:#f92672">=</span> question
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Configure the request parameters</span>
</span></span><span style="display:flex;"><span>    payload <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;model&#34;</span>: <span style="color:#e6db74">&#34;llama3.1&#34;</span>,  <span style="color:#75715e"># or whichever model you&#39;re using</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;prompt&#34;</span>: prompt,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;stream&#34;</span>: <span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;options&#34;</span>: {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;temperature&#34;</span>: <span style="color:#ae81ff">0.2</span>,
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># execute the HTTP request</span>
</span></span><span style="display:flex;"><span>    response <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>post(ollama_url, json<span style="color:#f92672">=</span>payload)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#lets see what we actually get back</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># print(response)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># return the text part of the respons</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> json<span style="color:#f92672">.</span>loads(response<span style="color:#f92672">.</span>text)[<span style="color:#e6db74">&#39;response&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assuming you have a .env file defined as described above</span>
</span></span><span style="display:flex;"><span>load_dotenv()
</span></span><span style="display:flex;"><span>ollama_host <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#39;OLLAMA_HOST&#39;</span>, <span style="color:#e6db74">&#39;127.0.0.1:11434&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>answer <span style="color:#f92672">=</span> ask_ollama(<span style="color:#e6db74">&#34;why is the sky blue&#34;</span>, ollama_host)
</span></span><span style="display:flex;"><span>print(answer)
</span></span></code></pre></div><p>And then we can call that file with</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python ollama_test.py  <span style="color:#75715e"># use the file name you chose for the above code</span>
</span></span></code></pre></div><p>When I run this on my server, using the <code>llama3.1</code> model, I get the following response:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>The sky appears blue to us because of a phenomenon called scattering, which occurs when sunlight interacts with the tiny molecules of gases in the atmosphere. Here&#39;s a simplified explanation:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>**Scattering and Rayleigh&#39;s Law**
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>When sunlight enters Earth&#39;s atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2). These molecules are much smaller than the wavelength of light, so they scatter the light in all directions.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>The amount of scattering that occurs depends on the wavelength of the light. Shorter wavelengths (like blue and violet) are scattered more than longer wavelengths (like red and orange). This is known as Rayleigh&#39;s Law, named after the British physicist Lord Rayleigh who first described it in 1871.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>**Why Blue Light is Scattered More**
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Blue light has a shorter wavelength (around 450-495 nanometers) compared to other colors. As a result, blue light is scattered more by the tiny molecules of gases in the atmosphere. This scattering effect makes the sky appear blue to our eyes.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>**Other Factors that Affect the Sky&#39;s Color**
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>While scattering is the main reason for the sky&#39;s blue color, there are other factors that can affect its appearance:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>* **Time of Day**: During sunrise and sunset, the light has to travel through more of the atmosphere, which scatters the shorter wavelengths (like blue) even more. This is why the sky often appears redder during these times.
</span></span><span style="display:flex;"><span>* **Atmospheric Conditions**: Dust, pollution, and water vapor in the air can scatter light in different ways, making the sky appear hazy or grayish.
</span></span><span style="display:flex;"><span>* **Earth&#39;s Atmosphere**: The atmosphere itself scatters light, but it also absorbs some of the longer wavelengths (like red), which is why we don&#39;t see as much red light from space.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>So, to summarize: the sky appears blue because of scattering by tiny molecules in the atmosphere, with shorter wavelengths like blue being scattered more than longer wavelengths.
</span></span></code></pre></div><p>Yay!  Too much information!</p>
<p>If you use a different model, or omit the <code>temperature</code> setting, you may see a different result.</p>
<p>While this was a success and acheived our overall goal of local LLM use, there are some downsides we will discuss below.</p>
<h3 id="the-python-library-approach">
  The Python Library approach
  <a class="heading-link" href="#the-python-library-approach">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Using the python library method is not quite as straight forward.  Each library has its own quirks and function definitions.  These are not always consistent and can lead to some frustrations as you need to dig deeper to understand specifics about the library you are trying to use.</p>
<p>I&rsquo;m using the <a href="https://pypi.org/project/ollama/"  class="external-link" target="_blank" rel="noopener">ollama</a> library for our examples here.  My experience with LangChain is not always a successful endeavor.  But, keep in mind the basic concepts we go over also apply to LangChain and other libraries.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> dotenv <span style="color:#f92672">import</span> load_dotenv
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> ollama
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">ask_ollama</span>(question, ollama_host<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;127.0.0.1:11434&#34;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Create the URL string for the REST request</span>
</span></span><span style="display:flex;"><span>    ollama_url <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;http://</span><span style="color:#e6db74">{</span>ollama_host<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Create a client object we will use to connect to our server</span>
</span></span><span style="display:flex;"><span>    client <span style="color:#f92672">=</span> ollama<span style="color:#f92672">.</span>Client(host<span style="color:#f92672">=</span>ollama_url)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Then we can call the appropriate method on that client.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Here we call .generate() as it is closest to the same process we did with the REST approach.</span>
</span></span><span style="display:flex;"><span>    response <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>generate(
</span></span><span style="display:flex;"><span>        model<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;llama3.1&#39;</span>, 
</span></span><span style="display:flex;"><span>        prompt<span style="color:#f92672">=</span>question)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Notice the response object is different than in the REST API method.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> response[<span style="color:#e6db74">&#34;response&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assuming you have a .env file defined as described above</span>
</span></span><span style="display:flex;"><span>ollama_host <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#39;OLLAMA_HOST&#39;</span>, <span style="color:#e6db74">&#39;127.0.0.1:11434&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> ollama_host:
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#39;could not determine the OLLAMA_HOST - trying the local URL&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>answer <span style="color:#f92672">=</span> ask_ollama(<span style="color:#e6db74">&#34;why is the sky blue&#34;</span>, ollama_host)
</span></span><span style="display:flex;"><span>print(answer)
</span></span></code></pre></div><p>The difference from the REST request approach is that the details of interacting with the Ollama system and its responses are understood by the <code>ollama</code> object.  So the object wraps up some of the coding effort and makes life easier for us.</p>
<p>HOW you specify the Ollama server is highly dependent on the library you are using.  In some cases, like above, a connection object is needed where the server details can be specified, and then the connection object is used to interact with the server via prompts/questions.  In other cases, the <code>base_url</code> or <code>host</code> parameter can be specified with every request/command method.  In some cases a specific environment variable may be needed.  You will need to understand the library you are choosing, which should be in the documentation for that library.</p>
<h2 id="downsides-to-hosting-locally">
  Downsides to hosting locally
  <a class="heading-link" href="#downsides-to-hosting-locally">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Both connection methods mentioned above will suffer from the same negative conditions:</p>
<ul>
<li>
<p>performance - the simple example question above saw my CPU utilization peg 100% for all my cores for approximately 60 seconds.  This is a direct result of</p>
<ul>
<li>The llama3.1 model.  It is a 4GB file that has to be fully loaded into memory and processed for every request.  This takes some time.  Using a smaller model will be more performant, but may not give results as good as the larger model.</li>
<li>My local hardware.  The better the hardware the Ollama server runs on the more performant the results will be.  In my case, I happen to be on a Ryzen 7 CPU with 16GB RAM, and an Nvidia GTX1650 graphics card.  My graphics card is a base model with GPU support and not as performant as a much newer card with better GPUs.  Your performance will not likely be the same with more modern hardware.</li>
<li>The configuration of the Ollama server.  When installing Ollama it attempts to find GPU support and configures this if available.  Otherwise Ollama is installed with CPU support only.  If that were the case for me, that one minute request might have taken 5 minutes instead, if the process didn&rsquo;t just crash.</li>
</ul>
</li>
<li>
<p>consistency - running the same questions multiple times may not give you the same results.  Adjusting the temperature setting, and perhaps specifying a &ldquo;seed&rdquo; value (alongside the temperature setting) could help but is not always a guarantee you&rsquo;ll get a consistent result.  The good news is that this is NOT an Ollama specific issue, or even a model specific issue.  Some models may be more consistent than others, but my experience thus far is that they all suffer this limitation.  (Disclaimer, my own experience should not in any way be defined as &ldquo;the standard&rdquo; to apply.)</p>
</li>
<li>
<p>dependability - I find that if I work my local Ollama server too hard things tend to become unstable.  In some cases I freeze my computer until the processing is done, in other cases my process may crash and take out the shell or editor window I&rsquo;m working in.  This is more noticable for the memory intensive tasks I might run.  The problem is that some questions and model combinations may not appear to be memory intensive until you are in the midst of process.</p>
</li>
</ul>
<p>Whether or not the performance issue is a concern depends on your task.  There are many tasks where delays just don&rsquo;t matter.  This is not an approach I would recommend for direct public website access, where 3 seconds might be too long for a response.  The issue may be mitigated through creative use of caching and other techniques though.</p>
<h2 id="conclusion">
  Conclusion
  <a class="heading-link" href="#conclusion">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Using a local Ollama server offers a number of potential opportunities for some interesting code.  Especially if you are looking into Retrieval Augmented Generation (RAG) where you want to be able to ask questions of your internal documentation.</p>
<p>Just like any other technology though, you need to be concious of the impacts and costs involved.</p>
<p>For myself, I have no hesitation using an external tool like <a href="https://openai.com/"  class="external-link" target="_blank" rel="noopener">OpenAI</a>, <a href="" >Google Gemini</a>, <a href="https://claude.ai/"  class="external-link" target="_blank" rel="noopener">Anthropic&rsquo;s Claude</a>, or <a href="https://www.meta.ai/"  class="external-link" target="_blank" rel="noopener">Meta AI&rsquo;s LLama models</a> - I can accept the risks for myself.  But if a customer asks me to use AI or LLMs to generate content for them, I would set up an internal system that does not expose customer data to external entities.  Not unless I have a written document from them indicating this is OK and they indemnify me against the liabilities for this.</p>
<p>The above shows two methods you might use to talk to an Ollama server to achieve that customer focused effort.</p>

      </div>


      <footer>
        


        
        
        
        
        
        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2020 -
    
    2024
     GroverCoder 
    
    
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>

</html>
