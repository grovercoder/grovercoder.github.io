<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Python on GroverCoder</title><link>https://grovercoder.github.io/tags/python/</link><description>Recent content in Python on GroverCoder</description><generator>Hugo</generator><language>en</language><lastBuildDate>Wed, 09 Oct 2024 16:01:10 -0600</lastBuildDate><atom:link href="https://grovercoder.github.io/tags/python/index.xml" rel="self" type="application/rss+xml"/><item><title>Using Ollama locally with Python</title><link>https://grovercoder.github.io/posts/2024108-local-ollama-python/</link><pubDate>Wed, 09 Oct 2024 16:01:10 -0600</pubDate><guid>https://grovercoder.github.io/posts/2024108-local-ollama-python/</guid><description>&lt;h2 id="overview">
 Overview
 &lt;a class="heading-link" href="#overview">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h2>
&lt;p>AI and LLMs are all the rage these days. Incorporating them into your code can open up a number of possibilities. Using the online resources / APIs have some issues though:&lt;/p>
&lt;ul>
&lt;li>your code may need to survive even if the online resource changes or is no longer available&lt;/li>
&lt;li>your code may be rate limited or blocked if thresholds are exceeded&lt;/li>
&lt;li>the token cost may accummulate much quicker than you were expecting. In practice that $0.0012 process is fine, once you go live you might find though that that process is run millions of times in a short time frame. (Perhaps this is a good problem to have?)&lt;/li>
&lt;li>there may be legal issues passing your data or intellectual property to an upstream provider who is not always clear how that data may be used.&lt;/li>
&lt;/ul>
&lt;p>Sometimes running a LLM process locally is a better choice.&lt;/p></description></item></channel></rss>